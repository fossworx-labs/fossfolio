<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <script src="https://cdn.tailwindcss.com" type="text/javascript">
  </script>
  <title>
   Speech Recognition with Vosk - A Tinkerer's canvas
  </title>
 </head>
 <body>
  <div class="font-serif">
   <div class="grid grid-cols-1 mx-3 lg:grid-cols-10 w-100 h-screen" id="navbar">
    <div class="h-100 overflow-hidden justify-center flex flex-col text-center col-span-1">
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/" id="Home">
      Home
     </a>
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/posts" id="Posts">
      Posts
     </a>
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/intro" id="About">
      About
     </a>
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/sitemap.xml" id="Sitemap">
      Sitemap
     </a>
    </div>
    <div class="border-r-2 border-l-2 col-span-7 overflow-y-scroll" id="content">
     <div class="border-b-2 my-2 p-3">
      <h1 class="text-4xl font-bold">
       No more Sphinx: Speech Recognition with Vosk
      </h1>
      <div class="py-3 my-3 text-2xl font-semibold text-gray-500" id="summary">
       The long-lived and long-loved CMU Sphinx, a brainchild of Carnegie Mellon University, is not maintained actively anymore, since 5 years. But does that mean that we need to move to more production-oriented solutions? No, we actually don't.
      </div>
      <div class="px-2 py-5 text-gray-700">
       <p id="published_on">
        Date Published: 2022-07-12 18:01:44
       </p>
       <p class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3" id="authors">
        Author(s):
                    
                        Anuran Roy¬†¬†
                    
                        ABCD
       </p>
       <img alt="Cover Image for Post" class="mx-auto" src="https://www.thesoftwarereport.com/wp-content/uploads/2021/03/Voice-Recognition-Technology.jpg"/>
      </div>
     </div>
     <div class="px-3 py-2 my-2 mx-3 space-y-3 text-xl">
      <h2 class="p-3 text-4xl font-bold" id="no-more-sphinx">
       No more Sphinx
      </h2>
      <p>
       The long-lived and long-loved CMU Sphinx, a brainchild of Carnegie Mellon University, is not maintained actively anymore, since 5 years. But does that mean that we need to move to more production-oriented solutions? No, we actually don't. The team CMU Sphinx Project has slowly rolled in a new child project -
       <a href="https://alphacephei.com/vosk">
        <strong>
         Vosk
        </strong>
       </a>
       .
      </p>
      <p>
       Note that there are many other production-oriented solutions available (like OpenVINO, Mozilla DeepSpeech, etc.), which are equally as good, if not better at speech recognition. I am focusing on the ease of setup and use. üòÖ
      </p>
      <h2 class="p-3 text-4xl font-bold" id="okay-i-dont-know-what-you-are-talking-about-please-explain-more">
       Okay, I don't know what you are talking about. Please explain more.
      </h2>
      <p>
       Quoting the
       <a href="https://cmusphinx.github.io/wiki/about/">
        <strong>
         Official CMU Sphinx wiki's About section
        </strong>
       </a>
       (forgive me for being lazy):
      </p>
      <p>
       <img alt="CMUSphinx Wiki About Section" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626621691957/5JfVUHJxE.png"/>
      </p>
      <h3 class="p-3 text-4xl font-bold" id="i-get-it-but-why-do-you-call-this-dead">
       I get it, but why do you call this dead?
      </h3>
      <p>
       This is the screenshot of the two most recent posts on the
       <a href="https://cmusphinx.github.io/">
        <strong>
         CMU Sphinx Official Blog
        </strong>
       </a>
       :
      </p>
      <p>
       <img alt="**CMUSphinx blog**" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626466904859/9OZ2VX4IU.png"/>
      </p>
      <p>
       Also this
       <a href="https://news.ycombinator.com/item?id=13040523">
        <strong>
         discussion
        </strong>
       </a>
       from YCombinator:
      </p>
      <p>
       <img alt="**YCombinator thread**" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626467333151/gAridT3-q.png"/>
      </p>
      <p>
       Even if I disagree with the YCombinator discussion, the official CMU Sphinx blog does little to give me confidence.
      </p>
      <h2 class="p-3 text-4xl font-bold" id="okay-i-get-it-so-what-now">
       Okay I get it. So what now?
      </h2>
      <p>
       Another screenshot from the main
       <a href="https://cmusphinx.github.io/">
        <strong>
         CMU Sphinx website
        </strong>
       </a>
       :
      </p>
      <p>
       <img alt="**CMUSphinx page head**" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626467505926/MDDCitxMU.png"/>
      </p>
      <p>
       Not gonna lie, I was pretty disappointed üôÅ. I've been a Sphinx user for quite sometime. I'm no researcher, but I was actually familiar with Sphinx. So I wondered how Vosk would do for me. And I was really surprised at the gentle learning curve to implement Vosk to my apps. But there is really less documentation at the time of writing this blog. I hope this post will fill up some of that gap.
      </p>
      <p>
       Anyways, enough chatter. So in this post, I am going to show you how to setup a simple Python script to recognize your speech, using it alongside NLTK to identify your speech and extract the keywords. The end result? A fully functional system that takes your voice input and processes it reasonably accurately, so that you can add voice control features to any awesome projects you may be building! üòÉ
      </p>
      <h2 class="p-3 text-4xl font-bold" id="setting-up">
       Setting up:
      </h2>
      <h3 class="p-3 text-2xl font-bold" id="stage-0-resolving-system-level-dependencies">
       Stage 0: Resolving system-level dependencies:
      </h3>
      <blockquote>
       <p>
        "Know thy tools." ~ Some great person.
       </p>
      </blockquote>
      <p>
       Okay so before I start, let's see with what we'll be working on:
      </p>
      <div class="list-decimal px-5">
       <li>
        A Linux System (Ubuntu in my case). Windows and Mac users, don't be disheartened - the programming part is the same for all.
       </li>
       <li>
        PulseAudio Audio Drivers
       </li>
       <li>
        Python 3.8 with pip working.
       </li>
       <li>
        A working Internet connection
       </li>
       <li>
        An IDE (preferably) (VSCode in my case)
       </li>
       <li>
        A microphone (or a headphone or earphone with an attached microphone)
       </li>
      </div>
      <p>
       So first, we need to install the appropriate
       <code>
        pulseaudio
       </code>
       ,
       <code>
        alsa
       </code>
       and
       <code>
        jack
       </code>
       drivers, among others.
      </p>
      <p>
       Assuming you're running Debian (or Ubuntu), type the following commands:
      </p>
      <pre><code>sudo apt-get install gcc automake autoconf libtool bison swig python3-dev
sudo apt-get install libpulse-dev jackd libasound2-dev
</code></pre>
      <p>
       <strong>
        Note
       </strong>
       : Don't try to combine the above 2 statements (no pro-gamer move now üòú).
       <code>
        libasound2-dev
       </code>
       and
       <code>
        jackd
       </code>
       require
       <code>
        swig
       </code>
       to build their driver codes.
      </p>
      <p>
       If you're familiar with CMU Sphinx, you'd realise that there are a lot of common dependencies - which is no coincidence. Vosk comes from Sphinx itself.
      </p>
      <p>
       <em>
        If you face some issues with installing
        <code>
         swig
        </code>
        , don't worry. Just Google your error with the keyword CMU Sphinx.
       </em>
      </p>
      <h3 class="p-3 text-2xl font-bold" id="stage-1-setting-up-vosk-api">
       Stage 1: Setting up Vosk-API
      </h3>
      <p>
       First, we need to download Vosk-API. The Vosk API needs less setup, compared to the original source code.
      </p>
      <p>
       Assuming you have git installed on your system, enter in your terminal:
      </p>
      <pre><code>git clone https://github.com/alphacep/vosk-api.git
</code></pre>
      <p>
       If you don't have git, or have some other issues with it,
       <a href="https://github.com/alphacep/vosk-api">
        <strong>
         download Vosk-API from here
        </strong>
       </a>
       .
      </p>
      <div class="list-decimal px-5">
       <li>
        Create a project folder (say speech2command). Download (or clone) the Vosk-api code into a subfolder there.
       </li>
       <li>
        Now extract the .zip file (or .tar.gz file) into your project folder (if you downloaded the source code as an archive).
       </li>
      </div>
      <p>
       Your directory structure should look something like this:
      </p>
      <pre><code>speech2command
     |_______ vosk-api
     |_______ ...
</code></pre>
      <p>
       Now we're good to go.
      </p>
      <h3 class="p-3 text-2xl font-bold" id="stage-2-setting-up-a-language-model">
       Stage 2: Setting up a language model
      </h3>
      <p>
       The versatility of Vosk (or CMUSphinx) comes from its ability to use models to recognize various languages.
      </p>
      <p>
       Simply put, models are the parts of Vosk that are language-specific and supports speech in different languages. At the time of writing, Vosk has support for more than 18 languages including Greek, Turkish, Chinese, Indian English, etc.
      </p>
      <p>
       <a href="https://alphacephei.com/vosk/models">
        <strong>
         The Vosk Model Wiki
        </strong>
       </a>
      </p>
      <p>
       In this post, we are going to use the
       <a href="http://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip">
        <strong>
         small American English model
        </strong>
       </a>
       . It's compact (around 40 Mb) and reasonably accurate.
      </p>
      <p>
       Download the model and extract it in your project folder. Rename the folder you extracted from the .zip file as
       <code>
        model
       </code>
       . Now, your directory structure should look like this:
      </p>
      <pre><code>speech2command
     |_______ vosk-api
     |_______ model
     |_______ ...
</code></pre>
      <p>
       Here is a video walkthrough (albeit a bit old):
      </p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/Itic1lFc4Gg" title="YouTube video player" width="560">
      </iframe>
      <p>
       <br/>
      </p>
      <h3 class="p-3 text-2xl font-bold" id="stage-3-setting-up-python-packages">
       Stage 3: Setting up Python Packages
      </h3>
      <p>
       For our project, we need the following Python packages:
      </p>
      <div class="list-decimal px-5">
       <li>
        platform
       </li>
       <li>
        Speech Recognition
       </li>
       <li>
        NLTK
       </li>
       <li>
        JSON
       </li>
       <li>
        sys
       </li>
       <li>
        Vosk
       </li>
      </div>
      <p>
       The packages
       <code>
        platform
       </code>
       ,
       <code>
        sys
       </code>
       and
       <code>
        json
       </code>
       come included in a standard Python 3 installation. We need to install the other packages manually.
      </p>
      <p>
       In the command line, type:
      </p>
      <pre><code>pip install nltk speech_recognition vosk
</code></pre>
      <p>
       Wait as the components get installed one by one.
      </p>
      <h3 class="p-3 text-2xl font-bold" id="stage-4-setting-up-nltk-packages">
       Stage 4: Setting up NLTK Packages
      </h3>
      <p>
       Now NLTK is a huge package, with a dedicated index to manage its components. We just downloaded the NLTK core components to get a basic program up and running. We need a few more NLTK components to add to continue with the code.
      </p>
      <p>
       The required packages are:
       <code>
        stopwords
       </code>
       ,
       <code>
        averaged_perceptron_tagger
       </code>
       ,
       <code>
        punkt
       </code>
       , and
       <code>
        wordnet
       </code>
       .
      </p>
      <pre><code>nltk.download("stopwords")
nltk.download("averaged_perceptron_tagger")
nltk.download("punkt")
nltk.download("wordnet")
</code></pre>
      <p>
       Or in one line:
      </p>
      <pre><code>nltk.download("stopwords", "averaged_perceptron_tagger", "punkt", "wordnet")
</code></pre>
      <h3 class="p-3 text-2xl font-bold" id="stage-5-programming-with-vosk-and-nltk">
       Stage 5: Programming with Vosk and NLTK.
      </h3>
      <p>
       Here comes the fun part! Let's code something in Python to identify speech and convert it to text, using Vosk-API as the backend.
      </p>
      <p>
       Make a new Python file (say
       <code>
        s2c.py
       </code>
       ) in your project folder. Now the project folder directory structure should look like:
      </p>
      <pre><code>speech2command
     |_______ vosk-api
     |_______ model
     |_______ s2c.py
     |_______ ...
</code></pre>
      <h4 class="p-3 text-xl font-bold" id="coding-time-now">
       Coding time now! ü§©
      </h4>
      <p>
       Okay, so the code for the project is given below. The code is pretty clean (or so I hope), and you can understand the code yourself (or just copy-paste it üòú).
      </p>
      <pre><code class="language-python">import platform
import speech_recognition as sr
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords, wordnet
import sys
import vosk
import json
from vosk import SetLogLevel

SetLogLevel(-1) # Hide Vosk logs

p = platform.system()


def listen():
    rec = sr.Recognizer()
    with sr.Microphone() as src:
        rec.adjust_for_ambient_noise(src)
        audio = rec.listen(src)
    try:
        cmd = rec.recognize_vosk(audio) # Connecting to Vosk API
    except Exception:
        print("Sorry, couldn't hear. Mind trying typing it?")
        cmd = input()
    return cmd


def pos_tagger(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return None


def lemmatizer(src):
    w = WordNetLemmatizer()
    pos_tagged = nltk.pos_tag(nltk.word_tokenize(src))
    wn_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))
    ls = []  # lemmatized sentence
    for word, tag in wn_tagged:
        if tag is None:
            ls.append(word)
        else:
            ls.append(w.lemmatize(word, tag))
    return ls


def make_tokens(lms):
    stop_words = set(stopwords.words('english')
    src3 = []
    for i in lms:
        if i in stop_words:
            pass
        else:
            src3.append(str(i)+" ")
    print("Keywords are:", end=' ')
    for i in src3:
        print(i, end=' ')


try:
    while True:
        print("\nSay some words: ")
        c = listen()
        print("Listened value=",c)
        d = json.loads(c)
        print("Command =", d["text"])
        if str(d["text"]).rstrip(" ") in ['stop', 'exit', 'bye', 'quit', 'terminate', 'kill', 'end']:
            print("\n\nExit command triggered from command! Exiting...")
            sys.exit()
        lemmatized = lemmatizer(d["text"])
        make_tokens(lemmatized)
except KeyboardInterrupt:
    print("\n\nExit command triggered from Keyboard! Exiting...")
</code></pre>
      <p>
       Now run this code, and this will set up a listener that works continuously - with some verbose logs as well - which you can see on your terminal screen. Ignore those logs, they are just for information.
      </p>
      <h4 class="p-3 text-xl font-bold" id="if-you-need-the-source-code-i-made-a-repo-for-it-vosk-demo">
       If you need the source code, I made a repo for it:
       <a href="https://github.com/anuran-roy/vosk-demo">
        <strong>
         Vosk Demo
        </strong>
       </a>
      </h4>
      <h3 class="p-3 text-2xl font-bold" id="explanation">
       Explanation:
      </h3>
      <p>
       Here is a flowchart that shows exactly how this works:
      </p>
      <p>
       <img alt="**Project mechanism flowchart**" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626590492431/US0gqw6pN.png"/>
      </p>
      <p>
       So this was it, folks! Enjoy your very own speech2text (or rather, speech2command) recognition system. Keep tinkering!
      </p>
      <p>
       Bye for now!
      </p>
      <p>
       <br/>
       <strong>
        <em>
         Ron
        </em>
       </strong>
      </p>
     </div>
     <div class="invisible py-5 my-5">
     </div>
    </div>
    <div class="sticky top-0 bottom-0 h-screen justify-center content-center align-center object-center flex flex-col col-span-2 text-center m-3" id="meta">
     <p class="flex justify-center items-center flex-col">
      <img alt="Author Picture" class="rounded-full py-5 my-5" src="https://pbs.twimg.com/profile_images/1421779872364564482/vZljyGHa_400x400.jpg" width="30%"/>
      <div class="text-2xl text-center font-bold">
       About
      </div>
      Lorem ipsum dolor sit, amet consectetur adipisicing elit. Accusantium, accusamus explicabo a veritatis culpa, expedita quos illo quibusdam earum mollitia molestiae ducimus sed sunt sit. Magnam earum quia est a iure voluptatem eligendi excepturi placeat facilis. Quaerat explicabo omnis consequatur officiis amet fugiat laboriosam quibusdam. Voluptatem obcaecati quos minus nostrum voluptatum accusantium sequi, aliquam repellendus, recusandae mollitia esse consequatur quidem. Vero impedit natus accusamus nobis blanditiis rerum quo corporis necessitatibus dolores recusandae iusto, temporibus a itaque obcaecati quisquam illo alias cum. Quibusdam odio porro officia, eos magnam eligendi sit voluptate rerum ipsam cum perspiciatis deleniti saepe reiciendis quam blanditiis iure!
     </p>
     <p class="flex flex-col lg:columns-3 columns-1 py-5 my-5">
      <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="http://github.com/fossworx-labs/">
       GitHub
      </a>
      <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="http://www.linkedin.com/in/anuran-roy/">
       LinkedIn
      </a>
      <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="http://twitter.com/AnuranRoy">
       Twitter
      </a>
     </p>
    </div>
   </div>
   <div class="fixed left-0 right-0 bottom-0 bg-gray-100 opacity-80 hover:opacity-100 p-3 align-center flex justify-center items-center flex-col" id="footer">
    <span class="text-gray-700 text-center">
     Using Blogtastic theme in
     <a class="font-bold text-blue-500" href="https://github.com/anuran-roy/fossfolio">
      FOSSfolio
     </a>
     . Made by
     <a class="font-bold text-blue-500" href="https://linktr.ee/anuranroy">
      Anuran
     </a>
     with üíô
    </span>
   </div>
  </div>
 </body>
</html>